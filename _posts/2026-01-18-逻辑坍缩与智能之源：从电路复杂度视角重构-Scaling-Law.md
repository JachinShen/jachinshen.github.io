---
layout: post
author: JachinShen
title:  "逻辑坍缩与智能之源：从电路复杂度视角重构 Scaling Law"
subtitle: "跳出 Transformer 范式，探索基于布尔化简的压缩智能"
date:   2026-01-18 14:00:00 +0800
categories: Study
tags: 
    - Study
    - MachineLearning
    - LogicMinimization
    - ScalingLaw
---

# [](#header-1)引子：量子化假设与逻辑门复杂度

苏剑林在[《基于量子化假设推导模型的尺度定律》](https://kexue.fm/archives/9607)中提出了一个深刻的假设：完美模型是由无数离散的“能力量子（Quanta）”组成的。模型能力的提升，本质上是参数量和数据量对这些量子的不断覆盖。

受此启发，结合密码学中对**逻辑门复杂度（Circuit Complexity）**的讨论，我开始思考：如果智能等同于压缩，那么学习的过程是否可以被建模为一个**大规模布尔逻辑化简**的过程？

在这个视角下，每一个训练样本都是布尔函数真值表上的一个点。这里有两个关键的技术细节：
1.  **Don't care (无关项)**：在构建真值表时，对于未见过的输入点，我们将其标记为 Don't care ("-")。这意味着这些点既可以是 0 也可以是 1。电路化简算法（如 Espresso）会利用这些自由度，自动选择最有利于缩短表达式的值。
2.  **模糊化简 (Approximate Synthesis)**：如果允许极少量数据点从 1 变为 0，就能换取电路规模的指数级下降。这种“有损压缩”本质上是将这些点识别为“噪音”并进行过滤。

泛化的本质，就是通过化简算法找到那个能够描述所有已知点、且复杂度最低的电路结构。

# [](#header-1)IGLM 模型：将“学习”定义为“化简”

我设计了一个实验性架构：**IGLM (Incrementally-Grown Logic Minimization Model)**。它完全抛弃了权重矩阵和浮点运算，回归到最纯粹的布尔代数。

### [](#header-2)核心机制

1.  **增长阶段 (Growth Phase)**：
    对于每一条新输入的训练数据 $(X, Y)$，模型通过增加新的逻辑项（类似于查找表 LUT 中的 Minterms）来无损地记录映射关系。这一阶段是纯粹的“死记硬背”，电路规模随数据量线性增长。

2.  **化简阶段 (Simplification Phase)**：
    利用 EDA 工业界的经典算法对逻辑项进行合并。

### [](#header-2)泛化的本质：逻辑坍缩

当算法发现多个离散的逻辑点可以被一个更简短的布尔表达式覆盖时，逻辑发生了“坍缩”。这种从“真值表”到“逻辑表达式”的突变，正是模型从记忆走向理解、产生泛化能力的瞬间。

# [](#header-1)实验：整数加法自动机的“逻辑进化”

为了验证这一构想，我设计了“整数加法自动机”实验：**不告知任何数学规则，让模型仅通过化简 $n$-bit 加法数据来自动学习“全加器”逻辑。**

### [](#header-2)核心实现

利用 `PyEDA` 库，我们可以方便地构建带无关项的真值表并调用 Espresso 算法：

```python
# 1. 初始化真值表，未见过的点标记为 Don't care ("-")
out_tts = []
for k in range(out_bits):
    data = ["-"] * (1 << (2 * num_bits))
    for a, b, c in train_data:
        idx = a | (b << num_bits) # 输入编码
        val = (c >> k) & 1        # 第 k 位的真值
        data[idx] = val
    
    # 2. 构建真值表对象
    tt = truthtable(inputs, data)
    out_tts.append(tt)

# 3. 调用 Espresso 算法进行逻辑最小化
minimized_exprs = espresso_tts(*out_tts)
```

### [](#header-2)实验数据

通过网格搜索得到的泛化准确率如下：

| Bits | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **2** | 12.5% | 14.3% | 40.0% | 25.0% | 0.0% |
| **3** | 37.5% | 38.5% | 45.0% | 38.5% | 85.7% |
| **4** | 57.8% | 63.1% | 66.2% | 69.2% | 76.9% |
| **5** | 78.5% | 74.6% | 79.9% | 77.6% | 80.6% |
| **6** | 86.2% | 86.5% | 87.6% | 88.9% | 93.4% |

在 6-bit 实验中，当数据覆盖率达到 0.9 时，准确率达到了 93.4%。观察化简后的表达式，模型已经自发地推导出了进位逻辑。

# [](#header-1)深度解析：用逻辑化简重构 Scaling Law

通过 IGLM 的实验观察，我们可以为 Scaling Law 中那些“玄学”现象找到坚实的计算机科学解释：

### [](#header-2)1. 参数量的本质：拒绝“过早优化”

模型参数量越大，意味着可容纳的逻辑门越多。这允许模型在训练初期先“死记硬背”下海量离散的真值点，而不急于进行合并。

这符合计算机科学中的**“不要过早优化（Don't premature optimize）”**原则。如果模型容量不足（小模型），它被迫在数据还没看全时就进行逻辑化简，这会导致模型为了强行拟合局部数据而化简出错误的“歪理”，从而无法发现全局的数学规律。

### [](#header-2)2. 蒸馏的奥秘：逻辑结构的直接迁移

为什么蒸馏得到的小模型强于直接训练的小模型？

因为直接训练小模型会再次陷入“过早优化”的困境。而蒸馏的过程，本质上是大模型将其已经化简完成、验证正确的**“精简电路结构”**直接拷贝给小模型。小模型不需要经历从冗余到化简的痛苦寻优过程，它只需作为载体承载这套现成的逻辑。

### [](#header-2)3. 数据质量：假阳性是化简的死敌

布尔化简的核心前提是逻辑项之间的相似性。噪声数据（假阳性）就像是在逻辑空间中随机撒下的“钉子”。在化简过程中，这些孤立的错误点会阻断正常逻辑支路的合并，导致电路复杂度异常升高，最小描述长度（MDL）大幅增加。

### [](#header-2)4. 数据顺序：从简单结构到复杂组合

复杂逻辑往往是由简单模块组合而成的。通过**课程学习（Curriculum Learning）**先喂简单数据，模型会先化简出稳定的“基础电路模块”。当面对复杂数据时，化简算法只需在现有模块之上进行增量组合，而非从零开始在巨大的搜索空间中寻找复杂结构。

# [](#header-1)总结

Scaling Law 不是玄学，它是计算复杂度在逻辑空间中的必然体现。

当我们跳出 Transformer 的浮点迷雾，回归到最朴素的布尔逻辑时，智能的轮廓反而变得更加清晰：**智能，就是那个能够解释世界的最简电路。**