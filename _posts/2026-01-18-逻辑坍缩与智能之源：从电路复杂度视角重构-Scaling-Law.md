---
layout: post
author: JachinShen
title:  "逻辑坍缩与智能之源：从电路复杂度视角重构 Scaling Law"
date:   2026-01-18 14:00:00 +0800
categories: Study
tags: 
    - Study
    - MachineLearning
    - LogicMinimization
    - ScalingLaw
---

# 背景：量子化假设与逻辑门复杂度

苏剑林在[《基于量子化假设推导模型的尺度定律》](https://kexue.fm/archives/9607)中提出了一个假设：模型能力的提升本质上是参数量和数据量对离散“能力量子（Quanta）”的覆盖。

结合密码学中对**逻辑门复杂度（Circuit Complexity）**的讨论，我尝试从布尔逻辑的角度去理解这个过程。如果将智能等同于压缩，那么学习过程可以被建模为一个**大规模布尔逻辑化简**的过程。在这个视角下，训练样本是布尔函数真值表上的点，而泛化则依赖于两个关键机制：

1.  **Don't care (无关项)**：对于未见过的输入点，在真值表中标记为 Don't care ("-")。化简算法（如 Espresso）会利用这些自由度，自动选择最有利于缩短表达式的值。
2.  **模糊化简 (Approximate Synthesis)**：允许极少量数据点发生翻转，以换取电路规模的指数级下降。这本质上是将部分点识别为“噪音”并进行过滤。

# IGLM 模型：将“学习”定义为“化简”

我设计了一个实验性架构 **IGLM (Incrementally-Grown Logic Minimization Model)**。它完全抛弃了权重矩阵和浮点运算，回归到布尔代数。

其核心机制分为两个阶段：
1.  **增长阶段 (Growth Phase)**：对于新输入的训练数据 $(X, Y)$，通过增加新的逻辑项（类似于查找表 LUT 中的 Minterms）来无损记录映射。此时电路规模随数据量线性增长，属于“死记硬背”。
2.  **化简阶段 (Simplification Phase)**：利用 EDA 算法对逻辑项进行合并。当多个离散点被一个简短的布尔表达式覆盖时，逻辑发生了“坍缩”，模型从记忆走向理解。

# 实验：整数加法自动机的逻辑进化

为了验证这一构想，我设计了“整数加法自动机”实验：在不告知任何数学规则的前提下，让模型仅通过化简 $n$-bit 加法数据来自动学习“全加器”逻辑。

### 核心实现

利用 `PyEDA` 库构建带无关项的真值表并调用 Espresso 算法：

```python
# 1. 初始化真值表，未见过的点标记为 Don't care ("-")
out_tts = []
for k in range(out_bits):
    data = ["-"] * (1 << (2 * num_bits))
    for a, b, c in train_data:
        idx = a | (b << num_bits) # 输入编码
        val = (c >> k) & 1        # 第 k 位的真值
        data[idx] = val
    
    # 2. 构建真值表对象
    tt = truthtable(inputs, data)
    out_tts.append(tt)

# 3. 调用 Espresso 算法进行逻辑最小化
minimized_exprs = espresso_tts(*out_tts)
```

### 实验数据

通过网格搜索得到的泛化准确率如下表所示。可以看到在 6-bit 实验中，当数据覆盖率达到 0.9 时，准确率达到了 93.4%，模型自发推导出了进位逻辑。

| Bits | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **2** | 12.5% | 14.3% | 40.0% | 25.0% | 0.0% |
| **3** | 37.5% | 38.5% | 45.0% | 38.5% | 85.7% |
| **4** | 57.8% | 63.1% | 66.2% | 69.2% | 76.9% |
| **5** | 78.5% | 74.6% | 79.9% | 77.6% | 80.6% |
| **6** | 86.2% | 86.5% | 87.6% | 88.9% | 93.4% |

# 深度解析：用逻辑化简重构 Scaling Law

基于 IGLM 的实验观察，可以为 Scaling Law 中的现象提供一种计算机科学解释：

### 1. 参数量的本质：拒绝“过早优化”
模型参数量越大，意味着可容纳的逻辑门越多。这允许模型在训练初期先记录海量离散真值点。如果模型容量不足（小模型），被迫在数据不足时进行化简，会导致模型为了拟合局部数据而产生错误的“逻辑”，无法发现全局规律。

### 2. 蒸馏的奥秘：逻辑结构的直接迁移
蒸馏过程本质上是大模型将其已经化简完成、验证正确的“精简电路结构”直接传递给小模型。小模型不需要经历从冗余到化简的寻优过程，只需作为载体承载这套现成的逻辑。

### 3. 数据质量：假阳性是化简的死敌
噪声数据（假阳性）在逻辑空间中表现为随机分布的孤立点。在化简过程中，这些点会阻断正常逻辑支路的合并，导致电路复杂度异常升高，最小描述长度（MDL）大幅增加。

### 4. 数据顺序：从简单结构到复杂组合
复杂逻辑往往由简单模块组合而成。通过课程学习（Curriculum Learning）先喂简单数据，模型会先化简出稳定的“基础电路模块”。面对复杂数据时，只需在现有模块上进行增量组合，降低了搜索空间的复杂度。

# 总结

Scaling Law 是计算复杂度在逻辑空间中的必然体现。当我们回归到布尔逻辑时，智能的轮廓变得清晰：它就是那个能够解释世界的最简电路。