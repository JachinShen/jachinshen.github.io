---
layout: post
author: JachinShen
title:  "GPT 翻译"
date:   2018-11-29 20:29:29 +0800
categories: Paper
tags: 
  - Paper
  - NLP
---

# GPT

## Improving Language Understanding by Generative Pre-Training

OpenAI

## 摘要

自然语言理解包括许多分散的任务。虽然无标签的文本语料很丰富，对于特定任务的有标签数据很稀少，使得分散训练的模型很难表现得足够好。我们证明了，通过在非监督离散语料上训练一个语言模型的通用预训练，可以取得极大提升。不同于之前的方法，我们在精调阶段利用了任务觉醒输入变换器，在尽可能少的改变模型的同时，提高了迁移的效率。我们证明了我们的方法在许多任务上的效果。我们的通用任务不可知模型，完全超过了拥有针对任务设计结构的模型，大大提高了十二个NLP任务中的九个最先进水平。

## 介绍

本文中，我们在语言理解任务上，探索了一种半监督的方法，组合了非监督预训练和监督精调。我们的目标是学习到一种通用的表示，只要稍微修改就可以在许多任务上迁移。我们假设，可以获取到大量的无标签语料和一些手工标准的数据集。我们的设置，不需要目标任务和无标签语料在同一个定义域。我们使用了两阶段的流程。首先，我们在无标签数据上，用语言模型作为目标，训练神经网络的初始参数。随后，我们在目标任务上，用相应的监督目标，调整这些参数。

关于模型结构，我们使用了变换器，一种在各种任务上表现优秀的结构。这个模型选择，比起RNN，提供了更架构化的记忆来处理文本中的长期依赖，使得不同任务间的迁移很稳定。在迁移期间，我们用遍历的方法，调整针对任务的特征。也就是，把结构化的文本输入，处理成单个连续的记号序列。我们在实验中证明，这些调整可以让我们用最小的变化来有效地精调预训练模型的结构。

## 相关工作

## 架构

我们训练的过程包括两个阶段。第一个阶段是在大量语料上学习一个高容量的语言模型。之后是精调阶段，这里我们用带标签数据，在一个特定任务上调整模型。

### 非监督预训练

给定一个非监督语料库的记号集合，我们使用标准的语言模型目标来最大化似然函数。

在实验中，我们使用多层变换器的变体，变换器解码器作为语言模型。这个模型在输入上下文记号加入位置信息，然后应用多头自注意操作，来生成目标记号的输出分布。

### 监督精调

预训练好模型，我们调整参数来适应监督目标任务。我们假设有一个带标签的数据集C，每个样本包括一个输入记号，和一个标签。输入扔进预训练模型，获取最后一层变换器的激活，然后放进另一层线性输出层来预测概率。这样就有一个新的似然函数目标。我们还发现了把语言模型作为精调的辅助目标，可以提高监督模型的泛化能力，加快收敛。事实上已经有工作观察到这一点了。

总之，精调时唯一多出来的参数是权重，和分割记号的嵌入。

### 特定任务输入的转化

对于一些任务，比如文本分类，我们可以直接精调我们的模型。其他模型可以用结构化的输入，比如有序句子对，文档问题回答三元组。既然我们的预训练模型已经在连续文本序列上训练过了，我们要作一些修改来适应任务。之前的工作把针对任务的结构放到迁移过的表示顶层。这种方法重新引入了一堆针对任务的个性化结构，而这些结构并没有经过迁移学习。与之相反，我们使用了一种遍历的方法。我们把结构化的输入转化成我们的模型可以处理的有序序列。这种输入的转换，使得我们可以对于跨框架的任务可以避免引入额外变化。我们提供了这种输入转换的概述。所有的转换都包括增加随机初始化的开端和结尾记号。

## 实验

## 分析

## 结论

我们引入了一种框架，可以用单个任务无关模型，通过广泛的预训练和分布的精调，得到很强的自然语言理解能力。通过在大量语料上用长文本预训练，我们的模型学习到了很多普遍的知识，并且可以处理长程依赖，之后成功地迁移到解决各种任务，提升了十二个任务中的九个最先进水平。使用非监督预训练来提升特定任务的性能很久以来就是机器学习研究的重要目标。我们的工作证明了这的确是可行的，并且给出了模型（变换器）和数据集（长依赖文本）合作的提示。我们希望这可以帮助最新的非监督学习研究，无论是对于自然语言理解，还是其他的领域，进一步提升我们对于非监督学习的理解。