---
layout: post
author: JachinShen
title:  "word2vec 翻译"
date:   2018-12-01 18:32:29 +0800
categories: Paper
tags: 
  - Paper
  - NLP
---

# word2vec

## Efficient Estimation of Word Representations in Vector Space

Tomas Mikolov Google Inc.

## 摘要

我们提出了两个全新的模型架构来从非常大的数据集中计算连续的词向量表达。这些表达的质量用一个单词相似性任务衡量，并且和之前基于不同种类神经网络的最强技术进行了对比。我们发现在大大提升准确率的同时，减小的计算成本。也就是，从16亿单词的数据集中学习高质量的词向量只用了不到一天。

另外，这些向量在衡量语法和语义相似性的测试中，取得了最先进的水平。

## 介绍

现在的许多NLP系统和技术把单词作为基本单位。词与词之间没有关系，只是词汇表中的一个索引。这种选择简单，鲁棒，并且在大量数据上训练过的这种简单模型，比在少量数据上训练的复杂数据性能好。一个例子就是用于概率语言模型的N-gram模型：可以在数十亿的单词上训练模型。

然而，这种简单的技术在许多任务遇到了瓶颈。比如，对于自动语音识别来说，相对在定义域的数据很有限，因此性能很大程度上由高质量的语音数据主导。在机器翻译中，现在对于许多语言的语料只有十几亿词，甚至更少。因此，有一些情况下，这种基本的技术，无法取得进展，因此我们关注更加高级的技术。

随着这些年来机器翻译技术的进展，在更大的数据集上训练更复杂的模型成为了可能。并且这些模型完全超过了简单的模型。也许最成功的概念是词的稠密表示。比如，NNLM超过了N-gram模型很多。

### 本文的目标

本文的主要目标是引入一种技术，这种技术可以用来从数十亿词的数据集中学习到高质量的数百万词向量。据我们所知，之前还没有工作，能够成功地在十亿词上训练出50到100维的词向量。

我们用最近提出的技巧来评估向量表示的质量，不仅希望同义词靠得近，还希望词有不同程度的相似性。这在之前屈折语言的上下文中观察到了。比如，名词有不同的结尾，如果我们在原始的向量空间里寻找次空间里相似的词，很有可能找到有相似结尾的词。

更令人惊奇的是，词向量的相似性超出了简单的语法标准。在词向量上使用简单的线性计算，可以发现King-Man+Woman得出的向量和Queen的向量最接近。本文中，我们开发了新的模型，在保持词直接线性正则性的同时，最大化这些向量操作的准确性。我们设计了一个新的理解性测试，来衡量语法和语义，并且说明了许多的正则性可以学习到很高的准确性。而且，我们讨论了训练时间和准确率受词向量维度和训练数据量的影响。

### 之前的工作

用连续变量表示单词由来已久。一个很流行的评估NNLM模型架构在2000年提出，其中一个包含线性映射层、非线性隐藏层的前传神经网络被用来同时学习词向量表示和概率语言模型。之后很多人继续这篇文章的工作。

另一个有趣的NNLM架构是先用只有一个隐藏层的神经网络学习词向量。接下来词向量用来训练NNLM。因此，词向量在没有构建出完整的NNLM时就训练好了。本文中，我们直接拓展了这种架构，并且关注第一步：用简单的模型训练词向量。

接下来会展示词向量可以用来大大提升许多NLP任务的性能。词向量本身的评估使用了不同的模型架构和语料。一些训练好的词向量在未来的研究很比较中很有用。然而，据我们所知，这些架构计算量很大，除了这种用了对角权重矩阵的对数双线性模型。

## 模型架构

有许多不同种类的模型用来评估连续的词表示，包括著名的LSA和LDA。本文中，我们关注神经网络学到的稠密的词表示，因为之前已经有人证明了，它比LSA更能保持词之间的线性正则性。LDA则在大数据集上计算量很大。

为了和不同的模型结构比较，我们先把一个模型的计算复杂度定义成训练整个模型需要的参数个数。其次，我们希望在最大化准确率的同时，最小化计算复杂度。

对于所有的下述模型，训练复杂度是O=ExTxQ，其中E是训练迭代数，T是训练集的单词数，Q是模型架构的复杂度。一般来说，E=3-50，T最多到十亿。所有的模型都使用SGD和BP训练。

### 前传神经网络

概率前传神经网络语言模型在2000年提出。它包括输入层、映射层、隐藏层和输出层。在输入层，N个之前的单词用V维独热编码，其中V是词汇表大小。输入层之后映射到一个NxD维的映射层P，使用共享的映射矩阵。每次只有N个输入激活，映射矩阵的分解相对来说轻松。

NNLM架构在映射层和隐藏层的计算很复杂，因为映射层中的值是稠密的。对于一个很普通的N取值10，映射矩阵的尺寸大约有500到2000，同时隐藏层的尺寸一般是500到1000个神经元。而且，隐藏层要计算词汇表中所有单词的概率分布，因此输出层有V维。因此，每次训练的计算复杂度是Q=NxD+NxDxH+HxV，其中主导项是HxV。然而，有很多方法可以避免。要么使用层次结构的softmax，要么用训练时非归一化的模型。用词汇表的二叉树型结构，输出单元数可以减少到log2(V)。因此，主要的复杂度是NxDxH项。

在我们的模型中，我们使用层次softmax，其中词汇表用霍夫曼树表示。这遵从了词频对于神经网络语言模型分类有帮助的发现。霍夫曼树用短编码表示高频词，这进一步减少了输出层的神经元数。不过这对于神经网络模型并不是很重要的加速，因为计算瓶颈在于NxDxH项。我们接下来会提出没有隐藏层的架构，因此严重依赖有效的softmax归一化。

### 循环神经网络模型

RNN可以克服NN的一些落点，比如不用制定上下文长度，并且理论上RNN可以比浅层神经网络表示更复杂的模式。RNN模型没有映射层，只有输入层、隐藏层和输出层。这种模型特别的地方在于循环矩阵可以把隐藏层连接到自己，从而使用延时连接。这使得RNN可以形成一些短期记忆，因为过去的信息可以用隐藏层的状态表示。

RNN模型的复杂度是Q=HxH+HxV

其中文本表示D和隐藏层有相同的维度。再一次，HxV可以减少到Hxlog2(V)。主要的复杂度来自HxH。

### 神经网络并行训练

## 新的对数线性模型

本节中，我们提出两个新的模型结构用来学习词的稠密表示，并最小化计算复杂度。从前一节我们可以观察到，最主要的复杂度来自模型中的非线性隐藏层。虽然这使得神经网络很有吸引力，但是我们决定用更简单的模型，虽然也许不能像神经网络那样精准地表示数据，但是可以在更多数据上有效地训练。

新的结构承接了我们之前的工作，其中我们发现了可以用两步来有效地训练神经网络语言模型：第一步，用简单的模型计算连续的词向量，然而让N-gram模型学习这些词的稠密表示。虽然之后有不少工作注重词向量的学习，我们认为这种方法最简单。需要注意的是，相关的模型很久之前就已经提出了。

### CBOW

第一个提出的结构和NNLM很像，不过非线性隐藏层移去了，而且所有词共享映射层，不止是映射矩阵。因此，所有的单词都映射到一个位置，也就是向量都平均了。我们称为词袋模型，因为历史中词的顺序对于映射层没有影响。另外，我们还使用了未来的词。下一节中，我们通过在四个过去和四个未来词上的对数线性模型，得到了最佳的性能。其中，训练标准是正确地分类中间的词。训练复杂度是Q=NxD+Dxlog2(V)

我们进一步发展出了CBOW模型，因为不同于传统的词袋模型，它使用上下文的连续稠密表示。需要注意的是，输入层和映射层的权重矩阵在所有单词位置贡献，和NNLM一样。

### Skip-gram

第二个模型和CBOW很像，但是不同于根据上下文预测当前单词，这个模型根据同一句中的另一个单词来最大化原来单词的分类。更准确的说，我们把每个当前单词作为输入，给一个有连续映射层的对数线性分类器，然后预测这个词前面后面一定范围的词。我们发现增加范围可以提升词向量的质量，但是也提高了计算复杂度。既然越远的词应该没那么大关系，我们可以给更少的权重，通过采样更少的这些词。

这种结构的计算复杂度是Q=Cx(D+Dxlog2(V))，其中C是词之间的最大距离。因此，如果我们选择C=5，训练每个词的时候，我们会随机选择1到C中的一个数R，然后用前面后面各R个词作为标签。这需要做Rx2个单词分类，也就是当前单词作为输入，R+R各单词作为输出。接下来的实验里，我们取C=10。

## 结果
为了比较不同版本词向量的质量，之前的文章一般都用一张表格来展示示例单词和他们最相似的单词，然后直观地理解他们。虽然单词Race和Italy相近很容易展示，但是在更复杂的相似性任务中比较这些向量就有挑战多了。我们承接了之前的观察，词之间有很多类型的相似性。比如，单词big应该和small和smaller相似那样和bigger相似。另一种相似性关系可以是单词对 big-biggest 和 small-smallest。

惊喜的是，这些问题可以用简单的词向量线性操作解决。为了寻找那个相对于small，和biggest相对于big的程度最相近的单词，我们可以计算向量X=biggest-big+small。然后，我们在向量空间里寻找和X最接近的向量，作为问题的答案。当词向量训练得很好的时候，用这种方法来找正确答案（smallest）是可行的。

最后，我们发现当我们在大数据上训练高维词向量时，得到的向量可以用来回来词之间的语义关系。比如城市和它所属的国家。比如，France对于Paris就像Germany对于Berlin。有这种语义关系的词向量可以用来提升现有的NLP任务。

### 任务描述

为了评估词向量的质量，我们定义了一个理解性的测试，包括了五种语义问题，和九种语法问题。

### 最大化准确率

### 模型结构比较

### 大规模并行计算模型

### 微软研究句子补全挑战

## 学习到相对关系的示例

## 结论

本文中我们研究了以各种模型在许多语法和语义语言任务上为导向的词向量表达的质量。我们观察到，用很简单的模型来训练高质量的词向量是可行的，比起流行的神经网络模型。因为更低的计算复杂度，可以从更大的数据集上学习高维词向量。使用DistBelief分布式框架，甚至可以在万亿数据集上训练CBOW和Skipgram模型，因为词汇表的尺寸几乎无限。

我们接下来的工作会证明词向量可以成功地用于其他工作。机器翻译的结果也看上去很有希望。接下来，将我们的工作和LDA比较会很有趣。我们相信理解性测试可以帮助研究社区提升现有评估词向量的技巧。我们也希望高质量的词向量可以成为未来NLP任务的重要一环。