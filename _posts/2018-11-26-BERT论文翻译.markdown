---
layout: post
title:  "BERT论文翻译"
subtitle: "Google还是牛啊"
date:   2018-11-26  22:05:00 +0800
categories: Paper
tags: 
    - Paper
    - NLP
---

# BERT

## Pre-training of Deep Bidirectional Transformers for Language Understanding

Google AI Language

## 摘要

我们引入了一种新的语言表示模型，BERT，基于变化器(Transformer)的双向文本编码表示。不同于最近的语言表示模型，BERT通过在所有层里联合左右的上下文信息，形成预训练的深度双向表示。从而，预训练的BERT表示可以只加入一个额外的输出层精调来创造最先进的模型。在结构不针对任务修改的情况下，该模型可以用于各种任务，比如问答系统、语言推断。

BERT概念上是很简单的，实际使用中是很有效的。它在十一个NLP任务中取得了最先进的成绩，在SQuAD v1.1中超过了人类水平两个百分点。

## 介绍

预训练的模型对于提升NLP任务的表现很有效。这些任务包括句子、词组、记号层次。

在下游任务上应用预训练的模型有两种策略：特征提取和精调。基于特征的方法，比如ELMo，模型结构针对任务设计，其中利用预训练模型的表示作为额外的特征。基于精调的方法，比如GPT，引入极小的针对任务的参数，并且只通过简单的精调预训练过的参数，来学习下游任务。在之前的工作中，这两种方法在预训练阶段都使用了相同的目标函数，并且他们都使用了没有方向的语言模型来学习普遍的语言表示。

我们认为现在的技术严重地限制了预训练模型的能力，特别是基于精调的方法。最主要的限制是，普通的语言模型是没有方向性的，这限制了预训练阶段可用的模型结构。比如，在OpenAI的GPT中，作者使用了一个从左到右的模型。每个记号只能注意到在变化器的自注意层前面的记号。这种限制对于句子层面的任务是次优的，而对于记号层面的任务，基于这种模型的精调将会是灾难性的，因为这种任务对于两边的上下文都很敏感。

本文中，我们改进了基于精调的方法，提出了BERT。BERT通过引入一个新的预训练目标：带掩码的语言模型，MLM，解决了之前提到的无方向性限制。MLM随机地掩盖输入中的记号，目标是通过上下文预测被掩盖词的词汇表索引。不同于从左到右模型的预训练，MLM有目的地允许文本表示融合左右的上下文。此外，我们也引入了一个预测下一句的任务，来同时训练文本对层面的表示。

本文的贡献如下：

- 我们展示了双向的预训练对于语言表示的重要性。不同于无向模型，BERT使用带掩码的语言模型，使得预训练的深度双向表示成为可能。这也和EMLo两个方向LSTM隐性的结合不同。
- 我们显示了预训练的表达削减了繁重的针对任务结构设计的需求。BERT是第一个在许多句子层面和记号层面，基于精调方法的先进模型。
- BERT刷新了十一个NLP任务的记录。模型双向的本质是最大的贡献。

## 相关工作

预训练语言表示的历史很长，我们简单地回顾一下。

### 基于特征的方法

数十年来，学习普遍可应用的文本表示是研究的热点区域。

这些方法已经被泛化到粗粒度，比如句嵌入或者段落嵌入。如同传统的词嵌入，这些学习过的特征，也被作为特征应用到下游模型中。

ELMo沿着不同的维度拓展了传统的词嵌入研究。他们提出了从语言模型中提取上下文敏感的特征。通过连接带有上下文信息的词嵌入和已存在的针对特定任务的模型，ELMo刷新了七个主要的NLP评估任务。

### 基于精调的方法

最近语言模型迁移学习的趋势是在一个语言模型目标上预训练一些模型结构，然后在有监督的下游任务中精调相同的模型。这种方法的优势在于只有很少的参数需要从头开始学习。至少部分由于这种优势，OpenAI GPT在GLUE句子层面任务上取得了最先进的结果。

### 利用有监督的数据迁移学习

非监督学习的优势是几乎有无限的数据可用，也有工作说明了从有大量数据的有监督任务迁移是有效的。除了NLP， 计算机视觉的研究也证明了从大型的预训练模型迁移学习的重要性。

## BERT

我们在这节介绍BERT和细节。我们首先纵览模型框架和BERT的输入表示。然后我们介绍预训练任务，这篇文章的核心创新。之后详细介绍预训练和精调的流程。最后，讨论BERT和GPT的不同之处。

### 模型结构

BERT的模型结构是一个多层的双向交换器编码器。由于最近交换器的使用无处不在，并且我们的应用和原始的极为相似，因此我们略过模型结构的背景介绍。

在本文中，L表示层数，H表示隐藏层尺寸，A表示自注意的头数。在所有情况中，我们设置前传/滤波器大小为4H。我们主要介绍两种尺寸的结果：

- BERT BASE: L=12, H=768, A=12, 110M
- BERT LARGE: L=24, H=1024, A=16, 340M

BERT BASE 和 GPT 有相等的模型大小。然而，BERT使用了双向的自注意，而GPT限制了交换器，只注意到之前的上下文。

### 输入表示

我们的输入表示可以在一个记号序列里随意地表示单句，或者句子对。对于给定的记号，输入表达构建过程为：求和，分段和位置嵌入。需要注意的是：

- 我们使用WordPiece嵌入，有30000个记号。我们用##来分割词片。
- 我们使用支持多达512记号的位置嵌入。
- 每个序列的第一个记号一直是特定的分类嵌入[CLS]，最后对应的隐藏状态是用于分类任务的表示。对于非分类任务，这个向量可以忽略。
- 句子对被封装到单个序列中。我们用两种方法分辨句子。首先，我们用一个特殊的记号分割[SEP]。其次，我们在第一个句子每个标记中加上一个学习过的A句嵌入，同样，在第二个句子加上B句嵌入。

### 预训练任务

不同于ELMo和GPT，我们不用传统的从左到右或者从右到左语言模型来预训练BERT，而是使用两种全新的非监督预测任务。

#### 任务一：带掩码的语言模型MLM

直觉上，一个深度双向模型应该比从左到右的单向模型，或者隐形双向模型强。然而，传统的语言模型只能训练一个方向，因为双向条件会让每个单词间接地在多层上下文中看到自己。

为了训练一个深度的双向表示，我们采用了简单直接的办法，以一定概率，在输入上随机掩盖一些记号，然后预测这些位置的词。我们称这种过程为MLM，虽然它也经常被称为Cloze任务。这种情况下，对应掩码的最后隐藏向量，可以扔进softmax，输出词汇表中的概率。在我们的实验中，我们在每个序列中随机掩盖了15%的记号。不同于对自动编码器降噪，我们只预测了掩码，而不是重建了整个输入。

虽然这样可以得到一个双向的预训练模型，这个方法有两个缺陷。第一是预训练和精调中有不匹配的地方，因为[MASK]记号在精调中是永远不会见到的。为了缓解这个现象，我们并不总是用[MASK]来替换单词，而是在选择15%的记号后：

- 并不一直用[MASK]替换单词
- 80%的时间：用[MASK]替换
- 10%的时间：用另一个随机的词代替
- 10%的时间：不变

变化器编码器并不知道要预测哪个单词，也不知道哪个单词被随机替换，所以它不得不对于每一个输入记号，都表示上下文信息。另外，因为随机替换在所有记号中只有1.5%，对于模型的理解能力应该没有什么坏处。

第二个缺陷是每个批次只有15%的记号被预测了，意味着需要更多步来收敛。MLM比从左到右的模型慢得多，但是实践中性能的提升，比训练成本重要。

#### 任务二：下一句预测

许多重要的下游任务，基于理解两句话之间的关系，然而语言模型并不能直接捕捉。为了训练理解句子关系的模型，我们预训练了一个二值化的下一句预测任务，可以从任何单语言的语料随意生成。需要注意的是，在选择句子A和B生成预训练例子的时候，50%的句子的确是下一句，50%是随机挑选的。

我们完全随机地选择非下一句的句子，最终的预训练模型在这项任务中得到了97%-98%的准确率。虽然很简单，但是对于QA和NLI很有效。

### 预训练流程

预训练的流程很大程度上遵循了现有的语言模型预训练的资料。我们选择了BooksCorpus和English Wikipedia作为预训练语料。对于维基百科，我们只提取了正文，忽略了列表，表格和页头。使用文档级别的语料，而不是句子层面的语料，对于提取长连续序列很重要。

为了生成每个训练用的输出序列，我们从语料中采样两段文本，虽然比起单句长得多，但是我们依然称为句子。第一句接受A嵌入，第二句接受B嵌入。50%的情况下，B是A的后句，另外50%的情况下，B是随机句，这样可以形成预测下一句任务。采样的时候，总长度小于512个记号。在用WordPiece转化后，随机遮盖15%。对于局部的词片不作特殊考虑。

我们以256个序列为一批次，训练了1000000步，大约在33亿语料上迭代了40次。我们使用了Adam优化器，最开始的10000步预热，之后学习率线性衰减。我们在所有层上使用了0.1的dropout。我们使用了gelu激活函数，而不是像GPT那样常见的relu。训练损失是平均掩盖语言模型极大似然与平均下一句预测极大似然之和。

### 精调流程

对于序列层次的分类任务，BERT精调很直观。为了获取定维的输入序列池化表示，我们取出第一个记号[CLS]的最终隐藏状态向量，记为C。精调时唯一加入的参数是一个分类层KxH的矩阵W，其中K是分类数，H是隐藏状态向量C的尺寸。标签的概率通过标准的softmax计算，$P=softmax(CW^T)$。BERT的所有参数和W都参与精调，最大化正确标签的log概率。对于跨区域和记号层次的预测任务，上述流程需要稍微根据具体任务修改。

精调的时候，大部分模型的超参数可以保持不变，除了批尺寸、学习率和迭代次数。dropout的概率一直保持在0.1。最佳的超参数应该和具体任务有关，不过我们发现下面的值已经很有效了：

- 批尺寸：16，32
- 学习率：5e-5，3e-5，2e-5
- 迭代次数：3，4

我们也发现，大规模的数据集对于超参数没有小数据集那么敏感。精调通常很快，所以在上述参数中穷尽搜索，再挑选出最佳模型，也是可行的。

## BERT和GPT的对比

BERT和GPT很有比较价值。GPT在大量语料上，训练了从左到右的变换器语言模型。事实上，BERT的很多设计特意贴合了GPT，以至于看上去这两个模型没有什么比较的地方。我们工作的核心论点在于两个新的预训练任务显著提升了性能。不过我们也注意到BERT和GPT训练时的不同之处：

- GPT在BooksCorpus上训练；BERT在BooksCorpus和Wikipedia上训练，数据集更大。
- GPT只在精调阶段引入了句子分割符和分类符；BERT在预训练的时候就学习到了。
- GPT一次训练32000个词，训练了一百万步；BERT一次训练了128000个词，训练了一百万步。
- GPT所有的精调任务，都使用了相同的学习率5e-5；BERT根据不同任务选择最合适的学习率。

为了分割出这些不同的影响，我们进行了控制变量实验。

# 实验

# 消融研究

# 结论

最近，使用语言模型迁移学习带来的巨大提升，已经证明了大量的无监督预训练是许多语言理解系统不可或缺的一环。事实上，即使是低资源消耗的任务，也可以从深度无向结构中受益。我们主要的贡献在于，拓展了这种发现，提出了深度双向模型，使得预训练模型成功地刷新了NLP任务。

虽然一些任务中的成绩很好看，甚至超过了人类的水平，但是还有很多工作要做，我们要研究那些也许没被BERT捕捉到的语言现象。