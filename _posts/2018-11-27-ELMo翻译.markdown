---
layout: post
author: JachinShen
title:  "ELMo翻译"
date:   2018-11-27 22:05:29 +0800
categories: Paper
tags: 
  - Paper
  - NLP
---
# ELMO

## Deep contextualized word representations

Allen AI, University of Washington

## 摘要

我们引入了一种新型的深度上下文化的文本表示模型，既可以表示文字使用的复杂特征，又可以根据上下文语境改变。我们的词向量是深度双向语言模型内部状态的函数，这个模型事先在大量语料上预训练过。这些表示可以很容易地添加到已知的模型中，并且显著地提升了六个NLP挑战的性能。我们也分析出，把预训练网络的深度内部状态显示出来，是很重要的，因为这使得下游的模型，可以使用多种半监督的信号。

## 介绍

## 相关工作

预训练的词向量，由于从大规模无标签文本中提取语义和语法信息的能力，已经成为了许多NLP任务的标准部件。然而，这种学习词向量的方法，只允许每个单词拥有单个上下文无关的表示。

已经有一些工作，尝试克服传统词向量的缺陷。有的通过次词级的信息丰富词向量，有的从不同的单词意思中学习不同的词向量。我们的方法也通过字符卷积获取次词级的信息，同时无缝地组合多重信息用于下游任务，而不是学着预测事先定义好的不同意思。

其他的工作也在学习上下文有关的表示。context2vec使用双向LSTM来编码一个单词周围的上下文。其他学习上下文嵌入的方法把单词自身也编入了表达中，然后用监督的机器翻译编码器、或者非监督的语言模型编码器计算。这些方法都受益于大数据集。本文中，我们充分利用了大量的单语言数据，在三千万句子组成的语料上训练我们的双向语言模型。我们也把这个方法拓展到了深度上下文化表示，在很多NLP任务上都表现良好。

深度双向RNN的不同层编码了不同种类的信息。比如，在深度LSTM的低层引入多任务语法监督，可以提高高层任务的性能。在基于RNN的编码器解码器机器翻译中，一个两层的LSTM编码器的第一层学习到的表示，比第二层在预测POS标签时更好。最后，LSTM的高层可以学习到单词的语义。我们也通过修改语言模型的目标，引导相似的信号。这对于下游任务利用各种类型的半监督模型很有益。

Dai等人使用语言模型预训练编码器解码器对，然后序列化自动编码器，最后用特定任务的监督来精调。相反，在使用无标签数据预训练双向语言模型后，我们锁定了权重，增加了针对任务的模型结构，从而增加大型通用模型对于下游小型任务的帮助。

## ELMo:语言模型嵌入

不同于流行的词嵌入，ELMO的文本表示是整个输入句子的函数。通过带字符卷积的双层双向语言模型计算所得，本质上是网络内部状态的线性函数。这种设置让我们可以进行半监督学习，先在大量数据上预训练模型，然后很容易地和大量NLP结构组合。

### 双向语言模型

给定N个记号的序列，一个前传语言模型，通过根据前k-1个词，建模第k个词的概率模型，计算序列的概率。

最佳的先进神经网络语言模型计算了独立于上下文的记号表示，然后输入到L层前传LSTM中。在每个位置，LSTM层输出一个上下文有关的表示。最高层的输出用于在softmax层预测下一个记号。

回传的语言模型和前传的很相似，除了从后往前运行，根据未来的上下文预测之前的记号。可以用和前传相似的办法来实现。

一个双向的语言模型，组合了前传和回传语言模型。我们的公式同时最大化前传和回传方向的对数极大似然。

我们把前传和回传中记号表示和Softmax层的参数绑定，LSTM的参数相互独立。总之，这个公式和Peter的方法很像，不过我们在两个方向之间共享了一些权重。在下一节，我们引入一种新的方法来学习一种新的文本表示，双向语言模型的线性表示。

### ELMo

ELMO是一种针对任务的双向语言模型中间层表示。对于每个记号，一个L层的双向语言模型计算一组2L+1的表达。

为了被包含进下游的模型，ELMO把所有层解构到一个向量。最简单的情况下，ELMO只选取顶层。更一般地，我们可以计算针对任务的所有层权重。

$$
\mathbf { E } \mathbf { L } \mathbf { M } \mathbf { o } _ { k } ^ { \operatorname { task } } = E \left( R _ { k } ; \Theta ^ { \operatorname { task } } \right) = \gamma ^ { \operatorname { task } } \sum _ { j = 0 } ^ { L } s _ { j } ^ { \operatorname { task } } \mathbf { h } _ { k , j } ^ { L M }
$$

其中，$s^{task}$是softmax归一化后的权重，并且缩放参数$\gamma^{task}$允许任务模型缩放整个ELMO向量。$\gamma$在实践中，对于优化流程的帮助很大。考虑到双向语言模型每层的激活函数，有不同的分布，一些情况下在每层应用层归一化也很有用。

### 在监督NLP任务上使用双向语言模型

给定预训练好的双向语言模型和一个目标NLP任务的监督结构，用双向语言模型来提升性能很容易。我们只需要跑一下双向语言模型，然后记录每个单词的所有层表示。接下来，我们让最终的任务模型，学习这些表示的线性组合。

首先，考虑不带双向语言模型的监督模型最低层。大多数的监督NLP模型共享最底层的结构，这让我们得以规范地引入ELMO。给定一个记号的序列，使用预训练的词嵌入来生成上下文独立的记号表示很容易，还可以加入字符基本的表示。接下来，模型会生成一个上下文敏感的表示，一般用双向RNN，CNN，或者前传神经网络。

想要把ELMO添加到监督模型，我们首先要冻结双向语言模型的权重，然后把ELMO的输出向量接到输入记号后面，把用ELMO增强过的文本表示扔进任务用的RNN。对于一些任务，我们发现增加另一个特殊的线性权重，用EMLO增强隐藏层可以取得更好的效果。即使监督模型的其他部分不动，这些改动也可以在更复杂的神经网络模型的上下文环境中发生。

最后，我们发现给ELMO加入一些dropout也很有用，还可以用$\lambda \| \mathbf { w } \| _ { 2 } ^ { 2 }$作为正则项。这使得ELMO权重的偏置可以接近所有双向语言模型的平均值。

### 预训练双向语言模型的架构

本文中的预训练双向语言模型和Jozefowicz等人的结构相似，不过经过了修改，增加了对双向同时训练的支持，并且在LSTM层之间增加了残差连接。本文关注大规模的双向语言模型，因为Peters等人特意说明了在只能前传的语言模型上加入双向，和大规模训练的重要性。

为了在保持纯净的字符级别表示的同时，平衡总的语言模型复杂度和对于下游任务的算力要求，我们对半削掉了最好的单模型CNN-BIG-LSTM的嵌入和隐藏层维数。最终的模型使用两层双向语言模型，每层有4096个单元和512维的映射，以及一个从第一层到第二层的残差连接。上下文迟钝的表示用了2048个字符级别词袋模型卷积滤波器，接上两个高速层，和一个线性映射降维到512个表达。总之，双向语言模型对于每个输入记号，包括由于纯粹的字符输入而在训练集外，提供了三层表达。相反，传统的词嵌入只对一个固定词汇表中的记号提供了一层表达。

在十亿单词上训练10个迭代后，平均的前传和回传困惑度为39.7，可以参考CNN-BIG-LSTM的30。总之，我们罚下前传和回传的困惑度几乎相同，不过回传的值会稍微低一些。

一旦预训练好了，双向语言模型可以计算任何任务的表示。一些情况下，在特定任务数据上精调双向语言模型，可以大幅降低困惑度，增加下游任务的性能。这可以看作一种双向语言模型的定义域迁移。因此，大多数情况下，我们在下游任务使用精调过的双向语言模型。

## 评测

## 分析

## 结论

我们引入了一种通用的方法来从双向语言模型中学习高质量深度上下文依赖文本表示。应用ELMO，可以在许多NLP任务上获得巨大提升。通过消融分析和其他控制变量实验，我们也确认了双向语言模型层有效地编码了语境中单词的不同类型的语法和语义信息，因此使用所有层可以提升任务的性能。